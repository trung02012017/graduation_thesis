\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{6}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problems}{6}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Summary}{6}{section.1.2}}
\citation{amiri2017survey}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Materials and background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:background}{{2}{7}{Materials and background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Cloud Computing}{7}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Auto-scaling problem}{7}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Well-known machine learning models for Auto-scaling in cloud computing}{7}{section.2.3}}
\newlabel{ml_timeseries}{{2.3}{7}{Well-known machine learning models for Auto-scaling in cloud computing}{section.2.3}{}}
\citation{azoff1994neural}
\citation{koskela1996time}
\newlabel{eq_ffnn_1}{{2.1}{8}{Well-known machine learning models for Auto-scaling in cloud computing}{equation.2.3.1}{}}
\citation{warsito2018cascade}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Multi-Layer Perceptrons (MLPs)}{9}{subsection.2.3.1}}
\newlabel{model_mlp}{{2.3.1}{9}{Multi-Layer Perceptrons (MLPs)}{subsection.2.3.1}{}}
\newlabel{eq_mlp_1}{{2.2}{9}{Multi-Layer Perceptrons (MLPs)}{equation.2.3.2}{}}
\newlabel{eq_mlp_2}{{2.3}{9}{Multi-Layer Perceptrons (MLPs)}{equation.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.1}Cascade Forward Neural Network (CFNN)}{9}{subsubsection.2.3.1.1}}
\newlabel{model_cfnn}{{2.3.1.1}{9}{Cascade Forward Neural Network (CFNN)}{subsubsection.2.3.1.1}{}}
\citation{zhang2000predicting}
\citation{connor1994recurrent}
\citation{chandra2012cooperative}
\citation{hochreiter1997long}
\citation{gers2002applying}
\citation{guo2016robust}
\citation{fu2016using}
\newlabel{eq_cfnn_1}{{2.4}{10}{Cascade Forward Neural Network (CFNN)}{equation.2.3.4}{}}
\newlabel{eq_cfnn_2}{{2.5}{10}{Cascade Forward Neural Network (CFNN)}{equation.2.3.5}{}}
\newlabel{eq_cfnn_3}{{2.6}{10}{Cascade Forward Neural Network (CFNN)}{equation.2.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Recurrent Neural Network (RNN)}{10}{subsection.2.3.2}}
\newlabel{model_rnn}{{2.3.2}{10}{Recurrent Neural Network (RNN)}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Short Term Memory (LSTM)}{10}{subsection.2.3.3}}
\citation{chung2014empirical}
\citation{langkvist2014review}
\citation{bone2000recurrent}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Traditional RNN architechture. Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\relax }}{11}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{model_lstm_gru}{{2.1}{11}{Traditional RNN architechture. Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\relax }{figure.caption.4}{}}
\newlabel{eq_lstm_1}{{2.7}{11}{Short Term Memory (LSTM)}{equation.2.3.7}{}}
\newlabel{eq_lstm_2}{{2.8}{11}{Short Term Memory (LSTM)}{equation.2.3.8}{}}
\newlabel{eq_lstm_3}{{2.9}{11}{Short Term Memory (LSTM)}{equation.2.3.9}{}}
\newlabel{eq_lstm_4}{{2.10}{11}{Short Term Memory (LSTM)}{equation.2.3.10}{}}
\newlabel{eq_lstm_5}{{2.11}{11}{Short Term Memory (LSTM)}{equation.2.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces LSTM and GRU architechture. Source: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\relax }}{12}{figure.caption.5}}
\newlabel{model_lstm_gru}{{2.2}{12}{LSTM and GRU architechture. Source: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Gated Recurrent Units (GRU)}{12}{subsection.2.3.4}}
\newlabel{eq_gru_1}{{2.12}{13}{Gated Recurrent Units (GRU)}{equation.2.3.12}{}}
\newlabel{eq_gru_2}{{2.13}{13}{Gated Recurrent Units (GRU)}{equation.2.3.13}{}}
\newlabel{eq_gru_3}{{2.14}{13}{Gated Recurrent Units (GRU)}{equation.2.3.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Fundamental knowledge}{13}{section.2.4}}
\newlabel{fun_know}{{2.4}{13}{Fundamental knowledge}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Artificial Neural Network (ANN)}{13}{subsection.2.4.1}}
\newlabel{ann_know}{{2.4.1}{13}{Artificial Neural Network (ANN)}{subsection.2.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.1}Activation functions}{13}{subsubsection.2.4.1.1}}
\newlabel{ann_act_func}{{2.4.1.1}{13}{Activation functions}{subsubsection.2.4.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.1.1}\textbf  {Sigmoid function}}{13}{paragraph.2.4.1.1.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.1.2}\textbf  {Hyperbolic Tangent (Tanh) function:}}{14}{paragraph.2.4.1.1.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.1.3}\textbf  {Rectified Linear Unit (ReLU) function:}}{14}{paragraph.2.4.1.1.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.1.4}\textbf  {Leaky ReLU function:}}{14}{paragraph.2.4.1.1.4}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.1.5}\textbf  {Exponential Linear Unit (ELU) function:}}{14}{paragraph.2.4.1.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.2}Loss functions}{15}{subsubsection.2.4.1.2}}
\newlabel{ann_loss_func}{{2.4.1.2}{15}{Loss functions}{subsubsection.2.4.1.2}{}}
\citation{rumelhart1988learning}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Backpropagation algorithm applied for FFNN with 1 hidden layer\relax }}{16}{algocf.1}}
\newlabel{algorithm_backprop}{{1}{16}{Backpropagation - the ANN Training Algorithm}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.3}Backpropagation - the ANN Training Algorithm}{16}{subsubsection.2.4.1.3}}
\newlabel{ann_backprop_alg}{{2.4.1.3}{16}{Backpropagation - the ANN Training Algorithm}{subsubsection.2.4.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.3.1}\textbf  {forward propagation phase}}{16}{paragraph.2.4.1.3.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.4.1.3.2}\textbf  {weights updating phase}}{17}{paragraph.2.4.1.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Swarm Optimization Algorithms}{17}{subsection.2.4.2}}
\newlabel{swarm_opt}{{2.4.2}{17}{Swarm Optimization Algorithms}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.1}Idea and motivation}{17}{subsubsection.2.4.2.1}}
\newlabel{opt_idea}{{2.4.2.1}{17}{Idea and motivation}{subsubsection.2.4.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.2}Particle Swarm Optimization (PSO)}{17}{subsubsection.2.4.2.2}}
\newlabel{pso_standard}{{2.4.2.2}{17}{Particle Swarm Optimization (PSO)}{subsubsection.2.4.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.3}Sea Lion Optimization Algorithm (SLnO)}{17}{subsubsection.2.4.2.3}}
\newlabel{slno_standard}{{2.4.2.3}{17}{Sea Lion Optimization Algorithm (SLnO)}{subsubsection.2.4.2.3}{}}
\citation{masadeh2019sea}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Sea Lion Optimization (SLnO)\relax }}{18}{algocf.2}}
\newlabel{algorithm_slno}{{2}{18}{Sea Lion Optimization Algorithm (SLnO)}{algocf.2}{}}
\newlabel{slno_eq1}{{2.16}{19}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.16}{}}
\newlabel{snlo_eq2}{{2.17}{19}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.17}{}}
\newlabel{snlo_eq3}{{2.18}{19}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.18}{}}
\newlabel{snlo_eq4}{{2.19}{19}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.19}{}}
\newlabel{snlo_eq5}{{2.20}{19}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.20}{}}
\newlabel{slno_eq6}{{2.21}{20}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.21}{}}
\newlabel{slno_eq7}{{2.22}{20}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.22}{}}
\newlabel{slno_eq8}{{2.23}{20}{Sea Lion Optimization Algorithm (SLnO)}{equation.2.4.23}{}}
\citation{masadeh2019sea}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Improved Sea Lion Optimization (ISLO) algorithm and Proposed Model for Auto-Scaling (ISLO-CFNN)}{21}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Improved Sea Lion Optimization (ISLO)}{21}{section.3.1}}
\newlabel{improved_ISLO}{{3.1}{21}{Improved Sea Lion Optimization (ISLO)}{section.3.1}{}}
\citation{eberhart1995particle}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Improved Sea Lion Optimization (ISLO)\relax }}{22}{algocf.3}}
\newlabel{algorithm_islo}{{3}{22}{Improved Sea Lion Optimization (ISLO)}{algocf.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Exploitation phase improvement}{22}{subsection.3.1.1}}
\newlabel{imprv_exploit}{{3.1.1}{22}{Exploitation phase improvement}{subsection.3.1.1}{}}
\citation{tizhoosh2005opposition}
\citation{tizhoosh2005opposition}
\citation{wang2007opposition}
\citation{tang2009enhanced}
\citation{rashid2010improved}
\citation{nguyen2019efficient}
\newlabel{islo_eq1}{{3.1}{23}{Exploitation phase improvement}{equation.3.1.1}{}}
\newlabel{islo_eq2}{{3.2}{23}{Exploitation phase improvement}{equation.3.1.2}{}}
\newlabel{islo_eq3}{{3.3}{23}{Exploitation phase improvement}{equation.3.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Exploration phase improvement}{23}{subsection.3.1.2}}
\newlabel{islo_eq4}{{3.4}{24}{Exploration phase improvement}{equation.3.1.4}{}}
\newlabel{imprv_explore}{{3.1.2}{24}{Exploration phase improvement}{equation.3.1.4}{}}
\newlabel{architecture}{{3.1.2}{24}{Exploration phase improvement}{equation.3.1.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{25}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{26}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {.1}References}{26}{section.0.1}}
\bibcite{amiri2017survey}{1}
\bibcite{azoff1994neural}{2}
\bibcite{bone2000recurrent}{3}
\bibcite{chandra2012cooperative}{4}
\bibcite{chung2014empirical}{5}
\bibcite{connor1994recurrent}{6}
\bibcite{eberhart1995particle}{7}
\bibcite{fu2016using}{8}
\bibcite{gers2002applying}{9}
\bibcite{Goodfellow-et-al-2016}{10}
\bibcite{guo2016robust}{11}
\bibcite{hochreiter1997long}{12}
\bibcite{koskela1996time}{13}
\bibcite{langkvist2014review}{14}
\bibcite{masadeh2019sea}{15}
\bibcite{nguyen2019efficient}{16}
\bibcite{rashid2010improved}{17}
\bibcite{rumelhart1988learning}{18}
\bibcite{tang2009enhanced}{19}
\bibcite{tizhoosh2005opposition}{20}
\bibcite{wang2007opposition}{21}
\bibcite{warsito2018cascade}{22}
\bibcite{zhang2000predicting}{23}
