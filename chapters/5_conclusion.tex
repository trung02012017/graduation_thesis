\documentclass[../main.tex]{subfiles}
\begin{document}
In this thesis, we proposed an improved version of Sea Lion Optimization algorithm called ISLO. As described in Section \ref{improved_ISLO}, ISLO it first improves the exploitation phase by using an idea from PSO, taking the individuals' best experience into consideration, and use 2 random parameters to balance the influences of the global best agent and individual's experience for enhancing exploitation ability of this algorithm. Secondly, a modification based on opposition-based learning is utilized in exploration phase. This method chooses opposite position of a random agent over the best agent instead of random existing one in current population, helping ISLO avoid the degradation in diversity of population over the course of iteration. ISLO performance is tested by experiments on 30 benchmark functions, and is compared with several well-known meta-heuristic optimization algorithms in Section \ref{sec:exp_theory}. The experimental results prove that the proposed algorithm is superior to other compared algorithms in the search capability. Therefore. it is evident that proposed modifications can provide better performance than the original SLnO in terms of accuracy and robustness.

On the other hand, we introduced a model based on ISLO and CFNN to solve the auto-scaling demand in cloud environments in Section \ref{sec:proposed_model}. In this model, ISLO plays an important role as an optimizer replacing traditional BP algorithm for training the neural network. ISLO-CFNN's performance is tested on 4 time-series datasets of workload, and compared with other well-known deep learning models, which are widely used in solving time-series forecasting problem. Also, ISLO's optimizing capability is experimented again and compared with other swarm-based algorithm in optimizing CFNN's parameters. The results provided in Section \ref{sec:exp_app} show that ISLO-CFNN provides very competitive results to the others in term of accuracy, even with a simple architechture of CFNN. 

Although this work tackles the problems SLnO faces, and significantly enhances performance of the algorithm, the convergence speed is still limited, so in the future, we will conduct research on the issue of accelerating the convergence speed of this algorithm. Furthermore, the improved algorithm will be applied to tuning hyper-parameters problem in deep learning models, which takes huge amount of time and resources.
\end{document}